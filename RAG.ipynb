{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeda885a-089a-4b61-a98b-7fb7e2884754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install chromadb ollama\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676e3598-78d4-4682-af74-8fd57d6bc581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raf/Projects/vub/current-trends-in-ai/challenge/.venv/lib64/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from chromadb import Documents, EmbeddingFunction, Embeddings, PersistentClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __call__(self, input: Documents) -> Embeddings:\n",
    "        batch_embeddings = embedding_model.encode(input)\n",
    "        return batch_embeddings.tolist()\n",
    "\n",
    "embed_fn = MyEmbeddingFunction()\n",
    "\n",
    "client = PersistentClient(path=\"./chromadb\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=f\"openml-translations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c91b5a1-cc9a-4bec-8bec-6b0d48b0f060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, uuid\n",
    "with open('data/data.jsonl') as f:\n",
    "    for line in f:\n",
    "        pair=json.loads(line)\n",
    "        inp=pair['input'] \n",
    "        output=pair['output']\n",
    "        embedding = embedding_model.encode(inp)\n",
    "\n",
    "        # upsert to chromadb\n",
    "        collection.upsert(\n",
    "            ids=[str(uuid.uuid1())],\n",
    "            metadatas=[dict(translation=output)],\n",
    "            documents=[inp],\n",
    "            embeddings=embedding.tolist(),\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7573ff3b-5e57-4571-b7ea-25bef0e96b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['dc68fa84-0f6d-11ef-b06b-b0dcefa5c64d',\n",
       "   'd63743f6-0ed6-11ef-9795-b0dcefa5c64d',\n",
       "   'd664404a-0ed6-11ef-9795-b0dcefa5c64d',\n",
       "   'd6c9b362-0ed6-11ef-9795-b0dcefa5c64d',\n",
       "   'd6b2d28c-0ed6-11ef-9795-b0dcefa5c64d',\n",
       "   'd708c9a8-0ed6-11ef-9795-b0dcefa5c64d']],\n",
       " 'distances': [[0.1591210514307022,\n",
       "   0.15912123024463654,\n",
       "   0.35951027274131775,\n",
       "   0.35951027274131775,\n",
       "   0.35951027274131775,\n",
       "   0.35951027274131775]],\n",
       " 'metadatas': [[{'translation': 'load dataset, calculate height using data'},\n",
       "   {'translation': 'load dataset, calculate height using data'},\n",
       "   {'translation': 'load dataset, calculate height using data and volume columns'},\n",
       "   {'translation': 'load dataset, calculate height using data and volume columns'},\n",
       "   {'translation': 'load dataset, calculate height using data and volume columns'},\n",
       "   {'translation': 'load dataset, calculate height using data and volume columns'}]],\n",
       " 'embeddings': None,\n",
       " 'documents': [['What is the height of the objects in the loaded dataset?',\n",
       "   'What is the height of the objects in the loaded dataset?',\n",
       "   'What is the height of objects with a specific volume in the loaded dataset?',\n",
       "   'What is the height of objects with a specific volume in the loaded dataset?',\n",
       "   'What is the height of objects with a specific volume in the loaded dataset?',\n",
       "   'What is the height of objects with a specific volume in the loaded dataset?']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " collection.query(\n",
    "    query_texts=[\"What is the height of the objects in the dataset?\"],\n",
    "    n_results=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d154d6-4fd7-4378-a07c-cce39f70f8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question(q):\n",
    "    return \"\"\"\n",
    "    [INST] \n",
    "    <<SYS>> \n",
    "    You are used by a software package as an API. So you should only respond with a direct answer.\n",
    "    \n",
    "    You should translate English sentences into sentences of a reduced English vocabulary, such that these result sentences can serve as input for an AutoML system.\n",
    "The reduced vocabulary consists of the following tokens: load, dataset, calculate, volume, cluster, clustering, regression, data, count, clusters, a, and, perform, using, id, surface, height, mass, density, columns.\n",
    "\n",
    "The sentences are queries or instructions regarding a dataset consisting of four columns: id (integer), surface (in square meters, float), height (in meters, float), mass (in kilograms, float). The 'id', 'surface', 'height' and 'mass' tokens of the reduced vocabulary correspond to these column names.\n",
    "These columns describe the id, surface, height and mass of each object. The volume of the object is not a column but can be calculated as the product of the surface and height. Density of the object can be calculated by dividing the mass by the volume of the object.\n",
    "\n",
    "You shouldn't do any calculations, only translate sentences to the reduced vocabulary while considering the relations between the colums of the dataset, as detailed out in the previous 2 sentences.\n",
    "From the input sentence you should deduce the type of machine learning algorithm. It should be either regression, clustering or none at all. \n",
    "\n",
    "As an example, a sentence such as `Given the objects dataset, how many types of objects can be found, according to their volume?` would be translated into `load dataset, calculate volume using the surface and height columns, cluster and count clusters`. In this case the required ML algorithm is clustering as there is no column indicating the object type. Instead, the object type is deduced by performing clustering, a type of unsupervised learning.\n",
    "Another example:  \"Train a model that can estimate density based on surface and mass given the objects dataset.\" is translated into \"load dataset, calculate density using surface and mass columns, and perform regression\". In this case the required ML algorithm is regression as density is an object property that is calculated by other columns and can thus be learned by regression, a supervised learning algorithm. \n",
    "    <</SYS>>\n",
    "    \n",
    "\n",
    "Can you translate the sentence \"%s\" and reply only with the translation?\n",
    "[/INST]\n",
    "\"\"\" %(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6641fcc-40b5-4853-a1c0-d856b7aff0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama2',\n",
       " 'created_at': '2024-05-11T12:43:30.500005365Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': ' Calculate: load dataset, calculate density using surface and mass columns, perform regression.'},\n",
       " 'done': True,\n",
       " 'total_duration': 55940056860,\n",
       " 'load_duration': 4191761435,\n",
       " 'prompt_eval_count': 563,\n",
       " 'prompt_eval_duration': 49942460000,\n",
       " 'eval_count': 18,\n",
       " 'eval_duration': 1803028000}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.chat(model='llama2', messages=[{'role': 'user', 'content': question(\"Calculate the density of the objects\")}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22261280-95e6-4bfc-a7ab-52126ac2737f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PersistentClient\n",
    "\n",
    "collection.upsert(\n",
    "        ids=batch_ids,\n",
    "        metadatas=batch_metadata,\n",
    "        documents=batch_titles,\n",
    "        embeddings=batch_embeddings.tolist(),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
